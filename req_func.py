import requests 
import sqlite3
from datetime import datetime
from bs4 import BeautifulSoup
from lxml import etree

def get_response_content(website):
    """ Send GET request to the website url (with headers set),
    add response object content into website dictionary with 'response'
    as key """
    headers = {
    'User-Agent': "Mozilla/5.0 (Windows NT 10.0; rv:109.0) "
        "Gecko/20100101 Firefox/115.0'"
        }
    response = requests.get(website['url'], headers=headers)
    website['response'] = response

def log_track_info(website):
    """ Keep track of scrapin' moment with a log file"""
    r = website['response']
    log_line = ( 
        f"scrap_time : {datetime.now()}, "
        f"url : {website['url']}, "
        f"status_code : {r.status_code}, "
        f"nbr_char_in_text : {len(r.text)}\n"
        )
    with open('log_file_scrap.txt', 'a') as log_file:
        log_file.write(log_line) 

def gen_sitename(website):
    """ Retrieve site name from url value into website dictionary
    then add to it with 'site_name' as key """
    site_name = website['url'].split(".")
    website['site_name'] = site_name[1]
    
def gen_html_filename(website):
    """ Generate file name to stock html content text of the page 
    Return 'html_filename' """
    html_filename = f"output_{website['site_name']}.html"
    if website['response'].status_code != 200:
        html_filename = f"corrupted_output_{website['site_name']}.html"
    return html_filename

def write_to_html_file(website):
    """ Create file with the name generated by function 
    gen_html_filename() then, write data to the file"""
    html_filename = gen_html_filename(website)
    with open(html_filename, 'w') as file:
        file.write(website['response'].text)

def open_doc(doc_name):
    with open(doc_name) as fp:
        text = fp.read()
        return text

def extract_content(website):
    """ Retrieve html doc, open it, then parse it with bs. Find body, 
    generate DOM tree then, use xpath strings from website dictionary to
    retrieve the specific content, for each article from kind of 
    'latest news' category : date, title and link"""
    doc_name = f"output_{website['site_name']}.html"
    doc = open_doc(doc_name)
    soup = BeautifulSoup(doc, "html.parser")
    body = soup.find("body")
    dom = etree.HTML(str(body))
    website['titles'] = dom.xpath(website['xpreq_title'])
    website['times'] = dom.xpath(website['xpreq_time'])
    website['links'] = dom.xpath(website['xpreq_link'])

def format_date(site):
    """ Format date to the standard format YYYY-MM-DD to keep 
    homogeneity into the .db file """
    date_formats = {
        'itpro': '%d %B %y',
        'darkreading': '%b %d, %Y',
        'thehackernews': '%b %d, %Y',
        'bleepingcomputer': '%B %d, %Y',
        'hackread': '%B %d, %Y',
        'gbhackers': '%B %d, %Y',
        'talosintelligence': '%B %d, %Y %H:%M'
    }
    if site['site_name'] in date_formats:
        format_str = date_formats[site['site_name']]
        site['times'] = [datetime.strptime(date, format_str).strftime('%Y-%m-%d') for date in site['times']]

def format_link(site):
    """ Format link to complete url if the link doesn't start 
    with http """
    for i in range(len(site['links'])):
        if not site['links'][i].startswith('http'):
            site['links'][i] = f"{site['url']}{site['links'][i]}"

def con_to_db():
    """ sqlite DB connection, if the file doesn't exist, it will be 
    created automatically """
    return sqlite3.connect('test_db.db')

def create_table() -> None:
    """ Create a table, if not exists, into the .db file. Uniqueness on 
    date, sitename and title to avoid duplicates into the file """
    con = con_to_db()
    c = con.cursor()
    query = (
        f"CREATE TABLE IF NOT EXISTS data_content "
        f"(date date, sitename text, title text, link text, "
        f"UNIQUE (date, sitename, title))"
        )
    c.execute(query)
    con.commit()
    c.close()
    
def insert_data_to_table(site) -> None:
    """ Aggregate elements with zip(), then format data as list and 
    send them to the database if not already into it (cfr uniqueness)"""
    con = con_to_db()
    c = con.cursor()
    data = list(zip(site['times'],
        [site['site_name']]*len(site['times']), 
        site['titles'],
        site['links']))
    query = (
        f"INSERT OR IGNORE INTO data_content "
        f"(date, sitename, title, link) VALUES (?, ?, ?, ?)"
        )
    for each in data:
        c.execute(query, each)
    con.commit()
    c.close()
